{"amount_correct": 11, "percentage_score": 61, "report_time": "2024-11-01 13:19:14", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this programming skill check.", "count": 3, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 3"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 10, "exact": true}, "status": true, "path": "../README.md"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 10 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fbc980eb4a0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_difference - AssertionError: Failed on case with identical \n     coverage reports\n     \n     test_question_one.py::test_compute_coverage_difference\n       - Status: Failed\n         Line: 28\n         Exact: [] == [CoverageItem...covered=True)] ...\n         Message: Failed on case with identical coverage reports\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_difference\n       Path: <...>/programming-skill-check-three-starter/exam/tests/test_question_one.py\n       Line number: 28\n       Message: AssertionError: Failed on case with identical coverage reports\n     assert [] == [CoverageItem...covered=True)]\n       \n       Right contains 3 more items, first extra item: CoverageItem(id=1, line='line1', covered=True)\n       \n       Full diff:\n       + []\n       - [\n       -     CoverageItem(id=1, line='line1', covered=True),\n       -     CoverageItem(id=2, line='line2', covered=True),\n       -     CoverageItem(id=3, line='line3', covered=True),\n       - ]\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 16\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_difference():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         item1 = CoverageItem(1, \"line1\", True)\n         item2 = CoverageItem(2, \"line2\", True)\n         item3 = CoverageItem(3, \"line3\", True)\n         item4 = CoverageItem(4, \"line4\", True)\n         item5 = CoverageItem(5, \"line5\", True)\n         item6 = CoverageItem(6, \"line6\", True)\n         item7 = CoverageItem(1, \"line1\", False)\n         item8 = CoverageItem(2, \"line2\", False)\n         item9 = CoverageItem(3, \"line3\", False)\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item1, item2, item3]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item4, item5, item6])\n             == []\n         ), \"Failed on case with no common coverage\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [\n             item2,\n             item3,\n         ], \"Failed on case with partial overlap\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item3, item2, item1]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_intersection([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item7, item8, item9])\n             == []\n         ), \"Failed on case with same ids but not covered\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item1, item2, item3]) == []\n         ), \"Failed on case with identical coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item4, item5, item6]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with no common coverage\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [item1], \"Failed on case with partial overlap\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item3, item2, item1]) == []\n         ), \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_difference([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item7, item8, item9]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with same ids but different coverage status\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f640694f080>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_generate_fuzzer_values - ValueError: empty range in randrange(32, 0)\n     \n     test_question_one.py::test_generate_fuzzer_values\n       - Status: Failed\n         Line: 319\n         Exact: ValueError\n         Message: empty range in randrange(32, 0)\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_generate_fuzzer_values\n       Path: <...>/programming-skill-check-three-starter/exam/tests/test_question_one.py\n       Line number: 319\n       Message: ValueError: empty range in randrange(32, 0)\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 87\n     @pytest.mark.question_one_part_b\n     def test_generate_fuzzer_values():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         max_length = 10\n         result = generate_fuzzer_values(max_length)\n         assert len(result) <= max_length, \"Generated string is too long\"\n         char_start = 65\n         char_range = 26\n         result = generate_fuzzer_values(100, char_start, char_range)\n         for char in result:\n             assert (\n                 char_start <= ord(char) < char_start + char_range\n             ), \"Character is not in range\"\n         result = generate_fuzzer_values(0)\n         assert result == \"\", \"Empty string not generated\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f06a8167050>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score - assert 1.0 == 0.0\n     \n     test_question_one.py::test_compute_mutation_score\n       - Status: Failed\n         Line: 113\n         Exact: 1.0 == 0.0 ...\n         Message: AssertionError\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score\n       Path: <...>/programming-skill-check-three-starter/exam/tests/test_question_one.py\n       Line number: 113\n       Message: assert 1.0 == 0.0\n      +  where 1.0 = compute_mutation_score([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 104\n     @pytest.mark.question_one_part_c\n     def test_compute_mutation_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # summary of the checks:\n         # check 1: Empty list of mutants\n         # check 2: Empty\n         # check 3: Partially detected\n         # check 4: Fully detected\n         # check 1: Empty list of mutants\n         assert compute_mutation_score([]) == 0.0\n         # check 2: All undetected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", False), Mutant(2, \"line2\", False)])\n             == 0.0\n         )\n         # check 3: Partially detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", False)])\n             == 0.5\n         )\n         # check 4: All detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", True)])\n             == 1.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "status": true}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "status": true}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "status": true}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 4, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "status": true}]}